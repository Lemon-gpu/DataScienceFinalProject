{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from pathlib import Path\n",
    "from torchtext.vocab import Vocab\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from tokenizers import Tokenizer\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from torchtext.data.metrics import bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # 初始化Shape为(max_len, d_model)的PE (positional encoding)\n",
    "        pe = torch.zeros(max_len, d_model).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "        # 初始化一个tensor [[0, 1, 2, 3, ...]]\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        # 这里就是sin和cos括号中的内容，通过e和ln进行了变换\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        # 计算PE(pos, 2i)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # 计算PE(pos, 2i+1)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # 为了方便计算，在最外面在unsqueeze出一个batch\n",
    "        pe = pe.unsqueeze(0)\n",
    "        # 如果一个参数不参与梯度下降，但又希望保存model的时候将其保存下来\n",
    "        # 这个时候就可以用register_buffer\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x 为embedding后的inputs，例如(1,7, 128)，batch size为1,7个单词，单词维度为128\n",
    "        \"\"\"\n",
    "        # 将x和positional encoding相加。\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, src_vocab, tgt_vocab, dropout=0.1):\n",
    "        super(TranslationModel, self).__init__()\n",
    "\n",
    "        # 定义原句子的embedding\n",
    "        self.src_embedding = nn.Embedding(len(src_vocab), d_model, padding_idx=2)\n",
    "        # 定义目标句子的embedding\n",
    "        self.tgt_embedding = nn.Embedding(len(tgt_vocab), d_model, padding_idx=2)\n",
    "        # 定义posintional encoding\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout, max_len=72) # 写死了最大长度为72\n",
    "        # 定义Transformer\n",
    "        self.transformer = nn.Transformer(d_model, dropout=dropout, batch_first=True)\n",
    "\n",
    "        # 定义最后的预测层，这里并没有定义Softmax，而是把他放在了模型外。\n",
    "        self.predictor = nn.Linear(d_model, len(tgt_vocab))\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"\n",
    "        进行前向传递，输出为Decoder的输出。注意，这里并没有使用self.predictor进行预测，\n",
    "        因为训练和推理行为不太一样，所以放在了模型外面。\n",
    "        :param src: 原batch后的句子，例如[[0, 12, 34, .., 1, 2, 2, ...], ...]\n",
    "        :param tgt: 目标batch后的句子，例如[[0, 74, 56, .., 1, 2, 2, ...], ...]\n",
    "        :return: Transformer的输出，或者说是TransformerDecoder的输出。\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        生成tgt_mask，即阶梯型的mask，例如：\n",
    "        [[0., -inf, -inf, -inf, -inf],\n",
    "        [0., 0., -inf, -inf, -inf],\n",
    "        [0., 0., 0., -inf, -inf],\n",
    "        [0., 0., 0., 0., -inf],\n",
    "        [0., 0., 0., 0., 0.]]\n",
    "        tgt.size()[-1]为目标句子的长度。\n",
    "        \"\"\"\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.size()[-1]).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "        # 掩盖住原句子中<pad>的部分，例如[[False,False,False,..., True,True,...], ...]\n",
    "        src_key_padding_mask = TranslationModel.get_key_padding_mask(src)\n",
    "        # 掩盖住目标句子中<pad>的部分\n",
    "        tgt_key_padding_mask = TranslationModel.get_key_padding_mask(tgt)\n",
    "\n",
    "        # 对src和tgt进行编码\n",
    "        src = self.src_embedding(src)\n",
    "        tgt = self.tgt_embedding(tgt)\n",
    "        # 给src和tgt的token增加位置信息\n",
    "        src = self.positional_encoding(src)\n",
    "        tgt = self.positional_encoding(tgt)\n",
    "\n",
    "        # 将准备好的数据送给transformer\n",
    "        out = self.transformer(src, tgt,\n",
    "                               tgt_mask=tgt_mask,\n",
    "                               src_key_padding_mask=src_key_padding_mask,\n",
    "                               tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "\n",
    "        \"\"\"\n",
    "        这里直接返回transformer的结果。因为训练和推理时的行为不一样，\n",
    "        所以在该模型外再进行线性层的预测。\n",
    "        \"\"\"\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def get_key_padding_mask(tokens):\n",
    "        \"\"\"\n",
    "        用于key_padding_mask\n",
    "        \"\"\"\n",
    "        return tokens == 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translation:\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model: nn.Module  = torch.load('model/model.pt', map_location=device)\n",
    "    enVocab: Vocab = torch.load('dataset/vocab_en.pt', map_location=device)\n",
    "    zhVocab: Vocab = torch.load('dataset/vocab_zh.pt', map_location=device)\n",
    "    tokenizer: Tokenizer = Tokenizer.from_file('bert-base-chinese/tokenizer.json')\n",
    "    maxLength = 72\n",
    "\n",
    "    def cut(self, src: str) -> list[torch.Tensor]:\n",
    "        result = []\n",
    "        for i in range(0, len(src), self.maxLength - 2):\n",
    "            temp = src[i: min(i + self.maxLength - 2, len(src))]\n",
    "            temp = torch.tensor([0] + self.enVocab(self.enTokenizer(temp)) + [1]).unsqueeze(0).to(self.device)\n",
    "            result.append(temp)\n",
    "        return result\n",
    "    \n",
    "    def translation(self, src: str) -> str:\n",
    "        # 将与原句子分词后，通过词典转为index，然后增加<bos>和<eos>\n",
    "        src = self.cut(src)\n",
    "        result = ''\n",
    "        for s in src:\n",
    "            tgt = torch.tensor([[0]]).to(self.device)\n",
    "            # 一个一个词预测，直到预测为<eos>，或者达到句子最大长度\n",
    "            for i in range(self.maxLength):\n",
    "                # 进行transformer计算\n",
    "                out = self.model(s, tgt)\n",
    "                # 预测结果，因为只需要看最后一个词，所以取`out[:, -1]`\n",
    "                predict = self.model.predictor(out[:, -1])\n",
    "                # 找出最大值的index\n",
    "                y = torch.argmax(predict, dim=1)\n",
    "                # 和之前的预测结果拼接到一起\n",
    "                tgt = torch.concat([tgt, y.unsqueeze(0)], dim=1)\n",
    "                # 如果为<eos>，说明预测结束，跳出循环\n",
    "                if y == 1:\n",
    "                    break\n",
    "            # 将预测tokens拼起来\n",
    "            tgt = ''.join(self.zhVocab.lookup_tokens(tgt.squeeze().tolist())).replace(\"<s>\", \"\").replace(\"</s>\", \"\")\n",
    "            result += tgt\n",
    "        return result\n",
    "    \n",
    "    def enTokenizer(self, text: str) -> list[str]:\n",
    "        return self.tokenizer.encode(text, add_special_tokens=False).tokens\n",
    "    \n",
    "    def __call__(self, text: str) -> str:\n",
    "        return self.translation(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_line_count(file: str) -> int:\n",
    "    count = 0\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def read_file(file: str, cache: str) -> list[list[str]]:\n",
    "    if cache is not None and os.path.exists(cache):\n",
    "        return torch.load(cache)\n",
    "    \n",
    "    chTokenizer = AutoTokenizer.from_pretrained('bert-base-chinese')\n",
    "    result = []\n",
    "    TranslationModel = Translation()\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        for line in tqdm(f, desc=\"translating:\", total=get_line_count(file)):\n",
    "            line = json.loads(line)\n",
    "            english: str = line['english']\n",
    "            chinese: str = line['chinese']\n",
    "            if chinese.isascii(): # if chinese is ascii, then it is english, which means we need to swap english and chinese\n",
    "                english, chinese = chinese, english\n",
    "            translated = TranslationModel(english)\n",
    "            # result.append([english, [chTokenizer.tokenize(chinese)], [[chTokenizer.tokenize(translated)]]])\n",
    "            result.append([english, chinese, translated])\n",
    "\n",
    "    if cache is not None and not os.path.exists(cache):\n",
    "        torch.save(result, cache)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "translating:: 100%|██████████| 20/20 [00:17<00:00,  1.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# data = read_file('data/functional_test.json', 'data/functional_test.pt')\n",
    "data = read_file('data/functional_test.json', None)\n",
    "# data = read_file('data/translation2019zh_valid.json', 'data/translation2019zh_valid.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: Slowly and not without struggle, America began to listen.\n",
      "Chinese: 美国缓慢地开始倾听，但并非没有艰难曲折。\n",
      "Translated: ①不要与结构结合，began来列出来，以免受关注。\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "English: I didn't own a Thesaurus until four years ago and I use a small Webster's dictionary that I'd bought at K-Mart for 89 cents.\n",
      "Chinese: 直到四年前我才有了一本词典。我使用的是用89美分在K市场里买来的一本韦氏小词典。我从来不使用单词处理程序。\n",
      "Translated: 迪尔瓦尼不过是一个300℃，多年前，而且200℃的时候使用一个小小的1500℃。\"R'Neddiuking那个'D'D'D'bougt,在30-1000℃,89cest的89cent。\"\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "English: portlet, you must write three short deployment descriptors: web.xml, portlet.xml, and geronimo-web.xml. (Some of these may have been generated by your IDE.)\n",
      "Chinese: portlet 之后，您必须编写三个简短的部署描述符：web.xml、portlet.xml 和 geronimo-web.xml（这其中的一些文件可能已经由 IDE 生成）。\n",
      "Translated: 你的portlet，你的Must写了三个stor的deployutordeployipoderipripoter:web.xmlp，p.xm.XML和Geronimo-web.xml.（Ortelt.xml），可能有可能贝恩·盖纳特纳提的XML（ENGenerationat）。依你的爱德华，依你的250.1∶2]，恩德，依照你的400英镑。\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "English: Dithering is a technique that blends your colors together, making them look smoother, or just creating interesting textures.\n",
      "Chinese: 抖动是关于颜色混合的技术，使你的作品看起来更圆滑，或者只是创作有趣的材质。\n",
      "Translated: 是一个技术性的技术，它会把你的颜色弄到到了魔法，把你的颜色弄得更加紧张。看上去像斯莫特勒，或者说只是在英塞尔默格文本里打印。\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "English: This paper discusses the petrologic characteristics of the coal-bearing strata under the geologic structural background of the Tertiary coal basin in Hunchun.\n",
      "Chinese: 本文以珲春早第三纪含煤盆地的地质构违背景为依据，分析了煤系地层的岩石学特征。\n",
      "Translated: 论文解释了柯贝林的共轭-熊白素的岩石色白内障。在GSRATA下面的GSTRATA下面的结构性结构，在10℃COCO的COARALL背地区。在1700℃的时候，巴士林在1400英镑的时候，巴斯辛在1000英镑的时候成了。\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "English: Women over 55 are pickier about their partners than at any other time in their lives.\n",
      "Chinese: 55岁以上的女人们对自己伴侣更为挑剔。\n",
      "Translated: 55岁以上的人比在ANYy的人要比在ANY的其他时间，比Picker更加有效。在IR中，在IRL活着，在活着的活着。\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "English: Ruben: So, to heal (with capital letters) you need to have no predilections.\n",
      "Chinese: 所以，要“治疗“他人你必须没有任何偏好。\n",
      "Translated: ①∶①，你们不知道有没有预置换代码的人（请使用资金）。提恩斯，提琴。蒂恩斯，。提琴斯，。\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "English: The second encounter relates to my grandfather's treasure box.\n",
      "Chinese: 第二次事件跟我爷爷的宝贝匣子有关。\n",
      "Translated: 第二次，Eneer重新到我的大发哥盒的Treasure盒。\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "English: Change the value for the <ejb-link> tag to MyEJB, which is the name of the EJB as defined in the ejb-jar.xml file.\n",
      "Chinese: 将 <ejb-link> 标记的值更改为 MyEJB，即在 ejb-jar.xml 文件中定义的 EJB 名称。\n",
      "Translated: \"[=ejb-link>标签到150℃,which是<ejb-link>标签的值,which是<ejb-linket>标签的名称。\"在EJB-JAR.xml文件中的DEFIDEFINED的文件中，XML文件中的DEFINED为DEFINED。\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "English: One way to address these challenges would be to establish a Truth and Reconciliation Commission modeled on the experience of Muggle South Africa.\n",
      "Chinese: 解决这些挑战的途径包括依照麻瓜在南非的经验设立真相与和解委员会。\n",
      "Translated: 要解决这些SeeHollenge的方法是要求一个ESTAB和一个ESTABBuine的ESTA。\"①\"\"Intheopethedextoftheof20005.\"\"\"()对OferthingestofthethextoftheodestheRICA.RCA....Ricas..ricia.RIA...ric.rica.rica....\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "English: Brain: If you don't mind, Jonathan, while you and Mr. Sun get acquainted, I'd like to check the arrangements for the meeting.\n",
      "Chinese: 如果你不介意，Jonathan，在你和孙先生互相认识时，我先失陪，看看会议安排得如何。\n",
      "Translated: \"\"\"你不介意,你不介意，,不介意,你和1∶3,600.okeationactionallinthead.\"Ed，（英国）想要查看的仲裁部分，以便为我的内尔顿查询。\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "English: Bailee Madison plays Sally, a young girl who goes to live with her father and his girlfriend.\n",
      "Chinese: 受托保管人麦迪逊扮演莎莉，一个年轻的女孩谁去与她同住的父亲和他的女朋友。\n",
      "Translated: \"Instors,一个年轻女孩，她和她的女孩子一起生活的时候。\"她和她的女孩子和她的女孩子都是朋友们在一起的。\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "English: Reduce blood fat, prevent thrombosis, arteriosclerosis, apoplexy and heart disease. Improve memory, nourish the brain and improve the intelligence.\n",
      "Chinese: 降血脂，预防血栓、动脉硬化、中风和心脏病；改善记忆，健脑益智。\n",
      "Translated: HOREDODFAT，HODT，HRHRVET，TOMSISCLISOS，APOPRINGUNDHTH。记忆体，不必要，不要忘记，不要忘记英特尔。塞维尼斯的内德，他的心灵，深深地深入到了一起，他的心理，他的心情是，他的。\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "English: Toomay said signs of community intolerance, including bumper stickers opposing same-sex marriage, also made him feel down, and he sought guidance from a school counselor after contemplating suicide.\n",
      "Chinese: Toomay 说到群体性不宽容的标志，其中包括保险杠贴纸里有反同性婚姻，这也让他觉得低落。 他在认真考虑过自杀后向一位学校咨询员寻求了指导。\n",
      "Translated: ①说有关于逗号不受忍耐，不宽容，无法忍受的标志。米杜里昂山素季，也使得希姆感到沉默，而他也使用了桂冠。一所学校的舞蹈从一个学校的余弦乐队中分裂，遂化了雪橇。\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "English: When you eat dinner out, reduce the temptation to clean your plate by setting aside one-third of your meal.\n",
      "Chinese: 当你在外吃晚餐的时候，把你晚餐中三分之一的食物放置在一边。\n",
      "Translated: ①你们不要忘掉了，重新开始把你的凝结到你们的凝结点的特夫亭重新发现。你的MEALAL中的一个T-Third的设置设置是你的ASED。\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "English: Sang Lan is one of the best athletes in our country.\n",
      "Chinese: 桑兰是我国最优异的运带动之一。\n",
      "Translated: ①我们国家的最好的阿特尔特斯是我们国家的一个最好的。\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "English: They are able to show that active peroxiredoxin 1, Prx1, an enzyme that breaks down harmful hydrogen peroxide in the cells, is required for caloric restriction to work effectively.\n",
      "Chinese: 他们已经证明出活性过氧化物酶1（prx1），一种能够将细胞内有害的过氧化氢分解的酶类，正是此正面效应的所需酶类。\n",
      "Translated: ①是一个活性骨氧化酶活性的THAHAIN，1，1，1，1，2，1，1，A酶活性THA。T断掉了细胞中的HRHROGENPENROXIDE，是为了重新查询问题。氯化重新调查工作时，勤奋工作的力度很低。\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "English: He went to slide upon the ice Before the ice would bear; Then he plunged in above his knees, Which made poor Simon stare.\n",
      "Chinese: 他到冰上去滑冰在冰还能支撑前； 接着他陷入水中直到膝盖， 可怜的西蒙睁大了眼。\n",
      "Translated: 在冰上，我们要把冰上浇在冰上，冰上压得落下来，他冲着熊；艾德在阿布夫尔的身上，发明了庞氏的淀粉，他们的钾，2000℃的Poore，200000℃的淀粉。\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "English: Miaoxiang son know the devil tricks, white three quick deployment.\n",
      "Chinese: 苗香儿知道了鬼子的诡计，白三部署速战速结。\n",
      "Translated: 苏恩知道德维尔·特里奇，白三快速去除了三个白三个。\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "English: “My daughter has been banned from watching the show, ” supermodel Cindy Crawford told ShowbizSpy.\n",
      "Chinese: 超级名模辛迪•克劳馥在接受美国ShowbizSpy网站采访时表示：“我禁止女儿看这个节目。\n",
      "Translated: \"WhederDaufter已经被BeanBaynbened,wheendoutheredemodel,Oneduerder，thereden,Y：Y：TO-LD_20℃，TELD=1000℃，TELDOLD_10℃，YY∶1300℃。\n",
      "------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(data)):\n",
    "    english, chinese, translated = data[i]\n",
    "    \n",
    "    print(f\"English: {english}\")\n",
    "    print(f\"Chinese: {chinese}\")\n",
    "    print(f\"Translated: {translated}\")\n",
    "    print(\"-\" * 114)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
